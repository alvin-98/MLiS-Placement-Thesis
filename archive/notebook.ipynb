{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", 'all', split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "# model_name = 'distilbert/distilgpt2'\n",
    "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'subject', 'choices', 'answer'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples=1\n",
    "ds = dataset.select(range(min(num_samples, len(dataset))))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'strategy': 'best_of_n','best_of': 5, 'max_new_tokens': 10, 'k': True, 'temperature': 1.0, 'model': }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, input_ids, attention_mask, args):\n",
    "    \"\"\"\n",
    "    Beam search decoding with num_beams.\n",
    "    \"\"\"\n",
    "    eos_id = model.config.eos_token_id\n",
    "    device = input_ids.device\n",
    "    beam_size = args.num_beams\n",
    "    seq_len = input_ids.size(1)\n",
    "    # initialize beam candidates\n",
    "    beams = [{\n",
    "        'ids': input_ids,\n",
    "        'mask': attention_mask,\n",
    "        'past': None,\n",
    "        'score': 0.0,\n",
    "        'done': False\n",
    "    }]\n",
    "    for _ in range(args.max_new_tokens):\n",
    "        all_candidates = []\n",
    "        for beam in beams:\n",
    "            if beam['done']:\n",
    "                all_candidates.append(beam)\n",
    "                continue\n",
    "            if past is None:\n",
    "                out = model(\n",
    "                    input_ids=generated,\n",
    "                    attention_mask=mask,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            else:\n",
    "                out = model(\n",
    "                    input_ids=generated[:, -1:],\n",
    "                    attention_mask=mask,\n",
    "                    past_key_values=past,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            logits = out.logits[:, -1, :]\n",
    "            past = out.past_key_values\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            topk_logprobs, topk_idx = torch.topk(log_probs, beam_size, dim=-1)\n",
    "            topk_logprobs = topk_logprobs[0]\n",
    "            topk_idx = topk_idx[0]\n",
    "            for j in range(beam_size):\n",
    "                next_tok = topk_idx[j].unsqueeze(0).unsqueeze(0)\n",
    "                new_ids = torch.cat([beam['ids'], next_tok], dim=-1)\n",
    "                new_mask = torch.cat([\n",
    "                    beam['mask'],\n",
    "                    torch.ones((1,1), dtype=beam['mask'].dtype, device=device)\n",
    "                ], dim=-1)\n",
    "                new_score = beam['score'] + topk_logprobs[j].item()\n",
    "                done = (next_tok.item() == eos_id)\n",
    "                all_candidates.append({\n",
    "                    'ids': new_ids,\n",
    "                    'mask': new_mask,\n",
    "                    'past': past,\n",
    "                    'score': new_score,\n",
    "                    'done': done\n",
    "                })\n",
    "        # select top beams\n",
    "        beams = sorted(all_candidates, key=lambda x: x['score'], reverse=True)[:beam_size]\n",
    "        if all(b['done'] for b in beams):\n",
    "            break\n",
    "    # pick best beam\n",
    "    best_beam = max(beams, key=lambda x: x['score'])\n",
    "    return best_beam['ids'][:, seq_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(model, input_ids, attention_mask, args):\n",
    "    \"\"\"\n",
    "    Greedy decoding: pick the highest-probability token at each step.\n",
    "    \"\"\"\n",
    "    eos_id = model.config.eos_token_id\n",
    "    device = input_ids.device\n",
    "    generated = input_ids\n",
    "    mask = attention_mask\n",
    "    past = None\n",
    "    for _ in range(args.max_new_tokens):\n",
    "        if past is None:\n",
    "            out = model(\n",
    "                input_ids=generated,\n",
    "                attention_mask=mask,\n",
    "                use_cache=True\n",
    "            )\n",
    "        else:\n",
    "            out = model(\n",
    "                input_ids=generated[:, -1:],\n",
    "                attention_mask=mask,\n",
    "                past_key_values=past,\n",
    "                use_cache=True\n",
    "            )\n",
    "        logits = out.logits[:, -1, :]\n",
    "        past = out.past_key_values\n",
    "        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=-1)\n",
    "        mask = torch.cat([\n",
    "            mask,\n",
    "            torch.ones((generated.size(0), 1), dtype=mask.dtype, device=device)\n",
    "        ], dim=-1)\n",
    "        if next_token.item() == eos_id:\n",
    "            break\n",
    "    return generated[:, input_ids.size(1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_of_n(model, input_ids, attention_mask, args):\n",
    "    \"\"\"\n",
    "    Run Best-of-N sampling: draw args.best_of samples via multinomial sampling\n",
    "    and pick the sequence with the highest sum of log-probabilities.\n",
    "    \"\"\"\n",
    "    eos_id = model.config.eos_token_id\n",
    "    device = input_ids.device\n",
    "    seq_len = input_ids.size(1)\n",
    "    best_seq = None\n",
    "    best_score = float('-inf')\n",
    "    for _ in range(args['best_of']):\n",
    "        # sampling pass\n",
    "        generated = input_ids\n",
    "        past = None\n",
    "        mask = attention_mask\n",
    "        for _ in range(args['max_new_tokens']):\n",
    "            if past is None:\n",
    "                out = model(\n",
    "                    input_ids=generated,\n",
    "                    attention_mask=mask,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            else:\n",
    "                out = model(\n",
    "                    input_ids=generated[:, -1:],\n",
    "                    attention_mask=mask,\n",
    "                    past_key_values=past,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            logits = out.logits[:, -1, :]\n",
    "            past = out.past_key_values\n",
    "            # apply temperature\n",
    "            logits = logits / args['temperature']\n",
    "            # top-k if requested\n",
    "            if args['k']:\n",
    "                k = 5\n",
    "                topk_vals, topk_idx = torch.topk(logits, k, dim=-1)\n",
    "                probs = torch.zeros_like(logits).scatter_(1, topk_idx, torch.softmax(topk_vals, dim=-1))\n",
    "            else:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "            mask = torch.cat([mask, torch.ones((generated.size(0), 1), dtype=mask.dtype, device=device)], dim=-1)\n",
    "            if next_token.item() == eos_id:\n",
    "                break\n",
    "        # score this sequence\n",
    "        with torch.no_grad():\n",
    "            full_out = model(input_ids=generated, attention_mask=mask, use_cache=False)\n",
    "            log_probs = torch.log_softmax(full_out.logits, dim=-1)\n",
    "        score = 0.0\n",
    "        for i in range(seq_len, generated.size(1)):\n",
    "            token_id = generated[0, i]\n",
    "            score += log_probs[0, i-1, token_id].item()\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_seq = generated[:, seq_len:]\n",
    "    return best_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_ids, attention_mask, args):\n",
    "    if args['strategy'] == 'best_of_n':\n",
    "        return best_of_n(model, input_ids, attention_mask, args)\n",
    "    elif args['strategy'] == 'greedy':\n",
    "        return greedy_search(model, input_ids, attention_mask, args)\n",
    "    elif args['strategy'] == 'beam_search':\n",
    "        return beam_search(model, input_ids, attention_mask, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d03bc86f24547bd821582e5e86cbe3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cyclic subgroup of Z_24 generated by 18 has order\n",
      "['4', '8', '12', '6']\n",
      "0\n",
      "(A) 4 (B) 8 (C) 12 (D) 6\n",
      "prompt: -length 178- You are an assistant. Read the question and answer with a single letter.\n",
      "Question: The cyclic subgroup of Z_24 generated by 18 has order\n",
      "Choices: (A) 4 (B) 8 (C) 12 (D) 6\n",
      "Answer:\n",
      "input_ids: lengthtorch.Size([1, 60]) tensor([[151646,   2610,    525,    458,  17847,     13,   4457,    279,   3405,\n",
      "            323,   4226,    448,    264,   3175,   6524,    624,  14582,     25,\n",
      "            576,  76002,  80115,    315,   1863,     62,     17,     19,   7907,\n",
      "            553,    220,     16,     23,    702,   1973,    198,  89283,     25,\n",
      "            320,     32,      8,    220,     19,    320,     33,      8,    220,\n",
      "             23,    320,     34,      8,    220,     16,     17,    320,     35,\n",
      "              8,    220,     21,    198,  16141,     25]])\n",
      "attention_mask: lengthtorch.Size([1, 60]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "inputs items dict_items([('input_ids', tensor([[151646,   2610,    525,    458,  17847,     13,   4457,    279,   3405,\n",
      "            323,   4226,    448,    264,   3175,   6524,    624,  14582,     25,\n",
      "            576,  76002,  80115,    315,   1863,     62,     17,     19,   7907,\n",
      "            553,    220,     16,     23,    702,   1973,    198,  89283,     25,\n",
      "            320,     32,      8,    220,     19,    320,     33,      8,    220,\n",
      "             23,    320,     34,      8,    220,     16,     17,    320,     35,\n",
      "              8,    220,     21,    198,  16141,     25]])), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))])\n",
      "inputs mapped {'input_ids': tensor([[151646,   2610,    525,    458,  17847,     13,   4457,    279,   3405,\n",
      "            323,   4226,    448,    264,   3175,   6524,    624,  14582,     25,\n",
      "            576,  76002,  80115,    315,   1863,     62,     17,     19,   7907,\n",
      "            553,    220,     16,     23,    702,   1973,    198,  89283,     25,\n",
      "            320,     32,      8,    220,     19,    320,     33,      8,    220,\n",
      "             23,    320,     34,      8,    220,     16,     17,    320,     35,\n",
      "              8,    220,     21,    198,  16141,     25]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[ 17,  15,  15,  11, 220,  17,  15,  15,  11, 220]])\n",
      "200, 200,\n"
     ]
    }
   ],
   "source": [
    "prompts, preds, trues = [], [], []\n",
    "for ex in tqdm(ds, desc=\"Generation\"):\n",
    "    question, choices, label = ex['question'], ex.get('choices', []), ex['answer']\n",
    "    print(question)\n",
    "    print(choices)\n",
    "    print(label)\n",
    "\n",
    "    choices_str = ' '.join(f\"({chr(65+i)}) {c}\" for i,c in enumerate(choices))\n",
    "    print(choices_str)\n",
    "\n",
    "    prompt = (\n",
    "            f\"You are an assistant. Read the question and answer with a single letter.\"\n",
    "            f\"\\nQuestion: {question}\\nChoices: {choices_str}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    print(\"prompt:\", f\"-length {len(prompt)}-\", prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    print('input_ids:', f\"length{inputs['input_ids'].shape}\", inputs['input_ids'])\n",
    "    print('attention_mask:', f\"length{inputs['attention_mask'].shape}\", inputs['attention_mask'])\n",
    "    print(\"inputs items\", inputs.items())\n",
    "    inputs = {k: v.to('cpu') for k,v in inputs.items()}\n",
    "    print(\"inputs mapped\", inputs)\n",
    "\n",
    "    # CUSTOM GENERATE FUNCTION\n",
    "\n",
    "    response = generate(model, inputs['input_ids'], inputs['attention_mask'], args)\n",
    "    print(response)\n",
    "    text = tokenizer.decode(response[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200, 200,'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
