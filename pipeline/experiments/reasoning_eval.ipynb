{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3dcfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import guidance\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc016a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lm = guidance.models.experimental.SglangModel(model=\"Qwen/Qwen3-30B-A3B-Instruct-2507\", base_url=\"http://127.0.0.1:30000/v1\" ,echo=True, api_key = os.environ.get(\"OPENAI_API_KEY\", \"NO_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ff706f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import guidance\n",
    "from guidance import models, system, user, assistant, json as gen_json\n",
    "import json\n",
    "\n",
    "class ReasoningPlan(BaseModel):\n",
    "    \"\"\"Model for the LLM-generated reasoning plan.\"\"\"\n",
    "    steps: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"A list of high-level, concise reasoning steps necessary to answer the query. Each step should be a clear action or question to address, without including detailed reasoning, sub-steps, or conclusions. Examples: 'Identify the subject of the sentence.', 'Analyze the key phrase in the context of the subject's domain.' Do not include a conclusion step.\"\n",
    "    )\n",
    "\n",
    "class ReasoningStep(BaseModel):\n",
    "    \"\"\"Model for an individual reasoning step execution.\"\"\"\n",
    "    observation: str = Field(\n",
    "        ...,\n",
    "        description=\"The result or observation from executing this reasoning step. Provide factual information, analysis, or findings relevant to the step, without jumping to overall conclusions.\"\n",
    "    )\n",
    "\n",
    "class ReasoningConclusion(BaseModel):\n",
    "    \"\"\"Model for the final reasoning conclusion.\"\"\"\n",
    "    conclusion: str = Field(\n",
    "        ...,\n",
    "        description=\"The final answer or conclusion to the query, based on all previous reasoning steps.\"\n",
    "    )\n",
    "\n",
    "@guidance()\n",
    "def _generate_reasoning_steps(llm):\n",
    "    \"\"\"\n",
    "    Generate a custom reasoning plan tailored to the query.\n",
    "    \"\"\"\n",
    "    with assistant():\n",
    "        llm += gen_json(\n",
    "            name=\"reasoning_plan\",\n",
    "            schema=ReasoningPlan,\n",
    "            max_tokens=300  # Reduced to encourage conciseness\n",
    "        )\n",
    "    return llm\n",
    "\n",
    "@guidance()\n",
    "def _intermediate_reasoning(llm, reasoning_plan_list):\n",
    "    with user():\n",
    "        llm += \"\"\"Follow the reasoning plan step-by-step. For each step, provide only the observation or result from executing it. Do not add extra details, sub-steps, or conclusions.\"\"\"\n",
    "\n",
    "    for i, step_description in enumerate(reasoning_plan_list):\n",
    "        # Set dynamic description for the field\n",
    "        ReasoningStep.model_fields[\"observation\"].description = f\"Execute this step: '{step_description}'. Provide the direct result or key findings.\"\n",
    "        \n",
    "        with assistant():\n",
    "            llm += gen_json(\n",
    "                name=f\"reasoning_step_{i}\",\n",
    "                schema=ReasoningStep,\n",
    "                max_tokens=200  # Limit to keep responses focused\n",
    "            )\n",
    "\n",
    "    return llm\n",
    "\n",
    "@guidance()\n",
    "def _reasoning_conclusion(llm):\n",
    "    with user():\n",
    "        llm += \"\"\"Now, based on all the previous steps, provide the final conclusion.\"\"\"\n",
    "    \n",
    "    with assistant():\n",
    "        llm += gen_json(\n",
    "            name=\"reasoning_conclusion\",\n",
    "            schema=ReasoningConclusion,\n",
    "            max_tokens=400\n",
    "        )\n",
    "\n",
    "    return llm\n",
    "\n",
    "@guidance\n",
    "def dynamic_structured_reasoning(llm, query):\n",
    "    with system():\n",
    "        llm += \"\"\"You are an expert reasoner. For any query, dynamically generate a concise list of intermediate reasoning steps tailored to the query. Execute each step to gather observations, then provide a final conclusion. \n",
    "        Structure all outputs as JSON. Create custom, high-level steps for the query without predefined templates. Keep steps action-oriented and free of details or conclusions.\"\"\"\n",
    "\n",
    "    with user():\n",
    "        llm += f\"\"\"Query: {query}\n",
    "        Generate a reasoning plan as a list of steps to resolve the query.\"\"\"\n",
    "\n",
    "    llm += _generate_reasoning_steps()\n",
    "\n",
    "    reasoning_plan = json.loads(llm.get(\"reasoning_plan\"))\n",
    "    reasoning_plan_list = reasoning_plan['steps']\n",
    "\n",
    "    llm += _intermediate_reasoning(reasoning_plan_list=reasoning_plan_list)\n",
    "\n",
    "    llm += _reasoning_conclusion()\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6649c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071f675b6f484b87bd297006775a32f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_lm = base_lm + dynamic_structured_reasoning(\n",
    "                query='Is the following sentence plausible? \"Bryce Harper hit the back shoulder fade.\"',\n",
    "\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52e78575",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_configs = ['boolean_expressions', 'causal_judgement', 'date_understanding', 'disambiguation_qa', 'dyck_languages', 'formal_fallacies', 'geometric_shapes', 'hyperbaton', 'logical_deduction_five_objects', 'logical_deduction_seven_objects', 'logical_deduction_three_objects', 'movie_recommendation', 'multistep_arithmetic_two', 'navigate', 'object_counting', 'penguins_in_a_table', 'reasoning_about_colored_objects', 'ruin_names', 'salient_translation_error_detection', 'snarks', 'sports_understanding', 'temporal_sequences', 'tracking_shuffled_objects_five_objects', 'tracking_shuffled_objects_seven_objects', 'tracking_shuffled_objects_three_objects', 'web_of_lies', 'word_sorting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21840757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'causal_judgement'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1943606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tasks: ['test']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the BBH dataset from Hugging Face\n",
    "dataset = load_dataset(\"lukaemon/bbh\", available_configs[1])\n",
    "\n",
    "# Print available tasks (keys) in the dataset\n",
    "print(\"Available tasks:\", list(dataset.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a285a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f5032b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How would a typical person answer each of the ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How would a typical person answer each of the ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How would a typical person answer each of the ...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How would a typical person answer each of the ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How would a typical person answer each of the ...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input target\n",
       "0  How would a typical person answer each of the ...     No\n",
       "1  How would a typical person answer each of the ...     No\n",
       "2  How would a typical person answer each of the ...    Yes\n",
       "3  How would a typical person answer each of the ...     No\n",
       "4  How would a typical person answer each of the ...    Yes"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3aeeac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No', 'Yes']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices = list(df.target.unique())\n",
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fdf1a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import guidance\n",
    "from guidance import models, system, user, assistant, json as gen_json, select\n",
    "import json\n",
    "\n",
    "class ReasoningPlan(BaseModel):\n",
    "    \"\"\"Model for the LLM-generated reasoning plan.\"\"\"\n",
    "    steps: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"A list of high-level, concise reasoning steps necessary to answer the query. Each step should be a clear action or question to address, without including detailed reasoning, sub-steps, or conclusions. Examples: 'Identify the subject of the sentence.', 'Analyze the key phrase in the context of the subject's domain.' Do not include a conclusion step.\"\n",
    "    )\n",
    "\n",
    "class ReasoningStep(BaseModel):\n",
    "    \"\"\"Model for an individual reasoning step execution.\"\"\"\n",
    "    observation: str = Field(\n",
    "        ...,\n",
    "        description=\"The result or observation from executing this reasoning step. Provide factual information, analysis, or findings relevant to the step, without jumping to overall conclusions.\"\n",
    "    )\n",
    "\n",
    "class ReasoningConclusion(BaseModel):\n",
    "    \"\"\"Model for the final reasoning conclusion.\"\"\"\n",
    "    conclusion: str = Field(\n",
    "        ...,\n",
    "        description=\"The final answer or conclusion to the query, based on all previous reasoning steps.\"\n",
    "    )\n",
    "\n",
    "from enum import Enum\n",
    "Options = Enum(\"Choices\", {choice: choice for choice in choices})\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    answer: Options = Field(..., description=\"Final answer.\")\n",
    "\n",
    "\n",
    "@guidance()\n",
    "def _generate_reasoning_steps(llm):\n",
    "    \"\"\"\n",
    "    Generate a custom reasoning plan tailored to the query.\n",
    "    \"\"\"\n",
    "    with assistant():\n",
    "        llm += gen_json(\n",
    "            name=\"reasoning_plan\",\n",
    "            schema=ReasoningPlan,\n",
    "            max_tokens=600  # Reduced to encourage conciseness\n",
    "        )\n",
    "    return llm\n",
    "\n",
    "@guidance()\n",
    "def _intermediate_reasoning(llm, reasoning_plan_list):\n",
    "    with user():\n",
    "        llm += \"\"\"Follow the reasoning plan step-by-step. For each step, provide only the observation or result from executing it. Do not add extra details, sub-steps, or conclusions.\"\"\"\n",
    "\n",
    "    for i, step_description in enumerate(reasoning_plan_list):\n",
    "        # Set dynamic description for the field\n",
    "        ReasoningStep.model_fields[\"observation\"].description = f\"Execute this step: '{step_description}'. Provide the direct result or key findings.\"\n",
    "        \n",
    "        with assistant():\n",
    "            llm += gen_json(\n",
    "                name=f\"reasoning_step_{i}\",\n",
    "                schema=ReasoningStep,\n",
    "                max_tokens=600  # Limit to keep responses focused\n",
    "            )\n",
    "\n",
    "    return llm\n",
    "\n",
    "@guidance()\n",
    "def _reasoning_conclusion(llm):\n",
    "    with user():\n",
    "        llm += \"\"\"Now, based on all the previous steps, provide the final conclusion.\"\"\"\n",
    "    \n",
    "    with assistant():\n",
    "        llm += gen_json(\n",
    "            name=\"reasoning_conclusion\",\n",
    "            schema=ReasoningConclusion,\n",
    "            max_tokens=600\n",
    "        )\n",
    "\n",
    "    return llm\n",
    "\n",
    "@guidance\n",
    "def dynamic_structured_reasoning(llm, query, output_choices):\n",
    "    with system():\n",
    "        llm += \"\"\"You are an expert reasoner. For any query, dynamically generate a concise list of intermediate reasoning steps tailored to the query. Execute each step to gather observations, then provide a final conclusion. \n",
    "        Structure all outputs as JSON. Create custom, high-level steps for the query without predefined templates. Keep steps action-oriented and free of details or conclusions.\"\"\"\n",
    "\n",
    "    with user():\n",
    "        llm += f\"\"\"Query: {query}\n",
    "        Generate a reasoning plan as a list of steps to resolve the query.\"\"\"\n",
    "\n",
    "    llm += _generate_reasoning_steps()\n",
    "\n",
    "    reasoning_plan = json.loads(llm.get(\"reasoning_plan\"))\n",
    "    reasoning_plan_list = reasoning_plan['steps']\n",
    "\n",
    "    with user():\n",
    "        llm += f\"\"\"\n",
    "        Execute the reasoning plan steps.\"\"\"\n",
    "\n",
    "    llm += _intermediate_reasoning(reasoning_plan_list=reasoning_plan_list)\n",
    "\n",
    "    with user():\n",
    "        llm += f\"\"\"\n",
    "        Make concluding statements.\"\"\"\n",
    "\n",
    "    llm += _reasoning_conclusion()\n",
    "\n",
    "    with user():\n",
    "        llm += f\"\"\"\n",
    "        Choose final answer.\"\"\"\n",
    "\n",
    "    with assistant():\n",
    "        llm += gen_json(\n",
    "            name=\"answer\",\n",
    "            schema=FinalAnswer,\n",
    "            max_tokens=600)\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "abf368cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(query, output_choices):\n",
    "\n",
    "    result_lm = base_lm + dynamic_structured_reasoning(\n",
    "                    query='Is the following sentence plausible? \"Bryce Harper hit the back shoulder fade.\"',\n",
    "                    output_choices = choices\n",
    "                )\n",
    "\n",
    "    answer = json.loads(result_lm.get(\"answer\"))['answer']\n",
    "    \n",
    "    reasoning_plan = json.loads(result_lm.get(\"reasoning_plan\"))['steps']\n",
    "\n",
    "    reasoning_steps = []\n",
    "    for i in range(len(reasoning_plan)):\n",
    "        reasoning_steps.append(json.loads(result_lm.get(f\"reasoning_step_{i}\"))['observation'])\n",
    "\n",
    "    reasoning_conclusion = json.loads(result_lm.get(\"reasoning_conclusion\"))['conclusion']\n",
    "    return reasoning_plan, reasoning_steps, reasoning_conclusion, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "18ef69b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How would a typical person answer each of the following questions about causation?\\nA machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the black wire cause the short circuit?\\nOptions:\\n- Yes\\n- No'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "989b3406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['target'] == result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2fbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = generate(df.iloc[0]['input'], choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5e99b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reasoning(row):\n",
    "    \"\"\"\n",
    "    Placeholder function to evaluate your reasoning system on a single row.\n",
    "    Replace with your dynamic_structured_reasoning function.\n",
    "    \"\"\"\n",
    "    query = row['input']\n",
    "    target = row['target']\n",
    "    # Your reasoning system should return a predicted answer\n",
    "    predicted = generate(query, choices)\n",
    "    return predicted == target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "602bd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['correct'] = df.apply(evaluate_reasoning, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b8591fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correct\n",
       "False    97\n",
       "True     90\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['correct'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import guidance\n",
    "from guidance import models, system, user, assistant, json as gen_json, select\n",
    "import json\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Strategy(BaseModel):\n",
    "    strategy: str = Field(\n",
    "        ...,\n",
    "        description=\"Carefully go over the problem statement and device a strategy to address the query. Do not delve into the details yet, just plan your general approach.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ReasoningPlan(BaseModel):\n",
    "    \"\"\"Model for the LLM-generated reasoning plan.\"\"\"\n",
    "    steps: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"Based on the strategy create a checklist of questions which must be answered to systematically arrive at the solution.\"\n",
    "    )\n",
    "\n",
    "class ReasoningStep(BaseModel):\n",
    "    \"\"\"Model for an individual reasoning step execution.\"\"\"\n",
    "    observation: str = Field(\n",
    "        ...,\n",
    "        description=\"Find the answer to the question in the checklist.\"\n",
    "    )\n",
    "\n",
    "class ReasoningConclusion(BaseModel):\n",
    "    \"\"\"Model for the final reasoning conclusion.\"\"\"\n",
    "    conclusion: str = Field(\n",
    "        ...,\n",
    "        description=\"The final answer or conclusion to the query, based on all previous reasoning steps.\"\n",
    "    )\n",
    "\n",
    "def dynamic_final_answer_schema(Options):\n",
    "    class FinalAnswer(BaseModel):\n",
    "        answer: Options = Field(..., description=\"Final answer.\")\n",
    "\n",
    "    return FinalAnswer\n",
    "\n",
    "\n",
    "@guidance()\n",
    "def _strategize(llm):\n",
    "    with assistant():\n",
    "        llm += gen_json(\n",
    "            name=\"strategy\",\n",
    "            schema=Strategy,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "    return llm\n",
    "\n",
    "@guidance()\n",
    "def _generate_reasoning_steps(llm):\n",
    "    \"\"\"\n",
    "    Generate a custom reasoning plan tailored to the query.\n",
    "    \"\"\"\n",
    "    with assistant():\n",
    "        llm += gen_json(\n",
    "            name=\"reasoning_plan\",\n",
    "            schema=ReasoningPlan,\n",
    "            max_tokens=3000  # Reduced to encourage conciseness\n",
    "        )\n",
    "    return llm\n",
    "\n",
    "@guidance()\n",
    "def _intermediate_reasoning(llm, reasoning_plan_list):\n",
    "    with user():\n",
    "        llm += \"\"\"This is the scratch pad\"\"\"\n",
    "\n",
    "    for i, step_description in enumerate(reasoning_plan_list):\n",
    "        # Set dynamic description for the field\n",
    "        ReasoningStep.model_fields[\"observation\"].description = f\"{step_description}\"\n",
    "        \n",
    "        with assistant():\n",
    "            llm += gen_json(\n",
    "                name=f\"reasoning_step_{i}\",\n",
    "                schema=ReasoningStep,\n",
    "                max_tokens=1000  # Limit to keep responses focused\n",
    "            )\n",
    "\n",
    "    return llm\n",
    "\n",
    "@guidance()\n",
    "def _reasoning_conclusion(llm):\n",
    "    with user():\n",
    "        llm += \"\"\"Based on the information, make logical deductions.\"\"\"\n",
    "    \n",
    "    with assistant():\n",
    "        llm += gen_json(\n",
    "            name=\"reasoning_conclusion\",\n",
    "            schema=ReasoningConclusion,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "\n",
    "    return llm\n",
    "\n",
    "@guidance\n",
    "def dynamic_structured_reasoning(llm, query, output_choices):\n",
    "    Options = Enum(\"Choices\", {choice: choice for choice in output_choices})\n",
    "    final_answer_schema = dynamic_final_answer_schema(Options)\n",
    "\n",
    "    with system():\n",
    "        llm += \"\"\"You are an expert in problem solving and a master strategist. For any query, strategize, draft an approach, create a checklist of intermediate steps which when followed can systematically solve the query. Execute each step to gather observations, then provide a final conclusion. \n",
    "        Structure all outputs as JSON. Create a general strategy without predefined templates. Use systems thinking and think from first principles. Keep steps action-oriented and free of details or conclusions.\n",
    "        Be very brief in your steps, tending towards concise but complete responses.\n",
    "        \"\"\"\n",
    "\n",
    "    with user():\n",
    "        llm += f\"\"\"Query: {query}\n",
    "        Devise a strategic plan to resolve the query -\n",
    "        \"\"\"\n",
    "\n",
    "    llm += _strategize()\n",
    "\n",
    "    with user():\n",
    "        llm += f\"\"\"\n",
    "        You are now given a scratch pad to workout a solution, in a list write down the title's of those scratch pads so you may use them in the next step -\n",
    "        \"\"\"\n",
    "\n",
    "    llm += _generate_reasoning_steps()\n",
    "\n",
    "    reasoning_plan = json.loads(llm.get(\"reasoning_plan\"))\n",
    "    reasoning_plan_list = reasoning_plan['steps']\n",
    "\n",
    "    with user():\n",
    "        llm += f\"\"\"\n",
    "        Find the answer to the question in the checklist -\n",
    "        \"\"\"\n",
    "\n",
    "    llm += _intermediate_reasoning(reasoning_plan_list=reasoning_plan_list)\n",
    "\n",
    "    with user():\n",
    "        llm += f\"\"\"\n",
    "        Make logical deductions based on the information you have to arrive at a conclusion -\n",
    "        \"\"\"\n",
    "\n",
    "    llm += _reasoning_conclusion()\n",
    "\n",
    "    with user():\n",
    "        llm += f\"\"\"\n",
    "        Your final answer -\n",
    "        \"\"\"\n",
    "\n",
    "    with assistant():\n",
    "        llm += gen_json(\n",
    "            name=\"answer\",\n",
    "            schema=final_answer_schema,\n",
    "            max_tokens=1000)\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e4698912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(query, output_choices):\n",
    "\n",
    "    result_lm = base_lm + dynamic_structured_reasoning(\n",
    "                    query=query,\n",
    "                    output_choices = output_choices\n",
    "                )\n",
    "\n",
    "    answer = json.loads(result_lm.get(\"answer\"))['answer']\n",
    "    \n",
    "    reasoning_plan = json.loads(result_lm.get(\"reasoning_plan\"))['steps']\n",
    "\n",
    "    reasoning_steps = []\n",
    "    for i in range(len(reasoning_plan)):\n",
    "        reasoning_steps.append(json.loads(result_lm.get(f\"reasoning_step_{i}\"))['observation'])\n",
    "\n",
    "    reasoning_conclusion = json.loads(result_lm.get(\"reasoning_conclusion\"))['conclusion']\n",
    "    return reasoning_plan, reasoning_steps, reasoning_conclusion, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c5c78228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the BBH dataset from Hugging Face\n",
    "dataset = load_dataset(\"WildEval/ZebraLogic\", 'mc_mode')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "036d6f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'puzzle', 'question', 'choices', 'answer', 'created_at'],\n",
       "        num_rows: 3259\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fdee8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reasoning(row):\n",
    "    \"\"\"\n",
    "    Placeholder function to evaluate your reasoning system on a single row.\n",
    "    Replace with your dynamic_structured_reasoning function.\n",
    "    \"\"\"\n",
    "    puzzle = row['puzzle']\n",
    "    question = row['question']\n",
    "    query = puzzle + '\\n\\n' + question\n",
    "    choices = row['choices']\n",
    "    target = row['answer']\n",
    "\n",
    "    # Your reasoning system should return a predicted answer\n",
    "    reasoning_plan, reasoning_steps, reasoning_conclusion, pred = generate(query, choices)\n",
    "    return {\n",
    "        'correct': pred == target,\n",
    "        'reasoning_plan': reasoning_plan,\n",
    "        'reasoning_steps': reasoning_steps,\n",
    "        'reasoning_conclusion': reasoning_conclusion,\n",
    "        'predicted_answer': pred,\n",
    "    }\n",
    "# reasoning_plan, reasoning_steps, reasoning_conclusion, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d3d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset['test'])\n",
    "# df.head()\n",
    "\n",
    "df = df.iloc[1:2]\n",
    "\n",
    "res = df.apply(lambda r: pd.Series(evaluate_reasoning(r)), axis=1)\n",
    "df = pd.concat([df, res], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "74100805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>puzzle</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lgp-test-3x6-15#mc-13</td>\n",
       "      <td>There are 3 houses, numbered 1 to 3 from left ...</td>\n",
       "      <td>What is Education of the person who lives in H...</td>\n",
       "      <td>[associate, high school, bachelor]</td>\n",
       "      <td>associate</td>\n",
       "      <td>2024-07-03T21:21:31.316147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                             puzzle  \\\n",
       "1  lgp-test-3x6-15#mc-13  There are 3 houses, numbered 1 to 3 from left ...   \n",
       "\n",
       "                                            question  \\\n",
       "1  What is Education of the person who lives in H...   \n",
       "\n",
       "                              choices     answer                  created_at  \n",
       "1  [associate, high school, bachelor]  associate  2024-07-03T21:21:31.316147  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e947d787",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'reasoning_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLiS-Placement-Thesis/pipeline/experiments/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'reasoning_steps'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreasoning_steps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLiS-Placement-Thesis/pipeline/experiments/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLiS-Placement-Thesis/pipeline/experiments/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'reasoning_steps'"
     ]
    }
   ],
   "source": [
    "df['reasoning_steps']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
