{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80aaa328-11f9-470f-b470-be8d79c20379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4,5,6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38c669b6-d032-4fbf-a51c-db2edbe850d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs01/home/ppxac9/MLiS-Placement-Thesis/pipeline/optimizing_inference/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 23:53:18 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 23:53:19,403\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM: 0.8.4 /gpfs01/home/ppxac9/MLiS-Placement-Thesis/pipeline/optimizing_inference/.venv/lib/python3.11/site-packages/vllm/__init__.py\n",
      "Torch: 2.6.0+cu124 12.4\n"
     ]
    }
   ],
   "source": [
    "import vllm, torch, os, subprocess, importlib\n",
    "print(\"vLLM:\", vllm.__version__, vllm.__file__)\n",
    "print(\"Torch:\", torch.__version__, torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144ec6ec-7fbd-49a1-b40b-70a1093bd06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 23:54:04 [config.py:689] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 08-11 23:54:04 [config.py:1713] Defaulting to use mp for distributed inference\n",
      "INFO 08-11 23:54:04 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 08-11 23:54:05 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='Qwen/Qwen3-30B-A3B-Instruct-2507', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-30B-A3B-Instruct-2507, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 08-11 23:54:05 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 08-11 23:54:05 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_9b69b794'), local_subscribe_addr='ipc:///tmp/20e39581-2428-4f9e-8f09-5b1fd442c42d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 08-11 23:54:06 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14d5bc6baed0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:06 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_67e06773'), local_subscribe_addr='ipc:///tmp/3a1ff34e-aef4-4b42-b1b6-464322671858', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 08-11 23:54:06 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14d5bc6c8c10>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m INFO 08-11 23:54:06 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1f9bbf5b'), local_subscribe_addr='ipc:///tmp/03e7e25c-adf3-42a3-95d4-5b47b220499d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 08-11 23:54:07 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14d5bc6c8410>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:07 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_335ca87b'), local_subscribe_addr='ipc:///tmp/6358310c-435a-4d60-a56c-1f7048181754', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 08-11 23:54:07 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14d5bc6c8b90>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:54:07 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7b747690'), local_subscribe_addr='ipc:///tmp/2b769e9f-8f61-49f7-ae3f-b1073c92e514', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:08 [utils.py:993] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:08 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:08 [utils.py:993] Found nccl from library libnccl.so.2\n",
      "INFO 08-11 23:54:08 [utils.py:993] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:08 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 08-11 23:54:08 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 08-11 23:54:08 [utils.py:993] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:54:08 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m WARNING 08-11 23:54:12 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 08-11 23:54:12 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 08-11 23:54:12 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 08-11 23:54:12 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:12 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_366a3511'), local_subscribe_addr='ipc:///tmp/b90ec359-e736-45de-a1eb-fca1f82b1c68', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:12 [parallel_state.py:959] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:12 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:12 [parallel_state.py:959] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:54:12 [parallel_state.py:959] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n",
      "INFO 08-11 23:54:12 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:12 [parallel_state.py:959] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 08-11 23:54:12 [gpu_model_runner.py:1276] Starting to load model Qwen/Qwen3-30B-A3B-Instruct-2507...\n",
      "INFO 08-11 23:54:12 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:12 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 08-11 23:54:12 [gpu_model_runner.py:1276] Starting to load model Qwen/Qwen3-30B-A3B-Instruct-2507...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:54:12 [gpu_model_runner.py:1276] Starting to load model Qwen/Qwen3-30B-A3B-Instruct-2507...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:12 [gpu_model_runner.py:1276] Starting to load model Qwen/Qwen3-30B-A3B-Instruct-2507...\n",
      "WARNING 08-11 23:54:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m WARNING 08-11 23:54:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m WARNING 08-11 23:54:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m WARNING 08-11 23:54:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:13 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:54:13 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:13 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m INFO 08-11 23:54:13 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/16 [00:00<00:11,  1.26it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/16 [00:01<00:10,  1.30it/s]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 3/16 [00:02<00:09,  1.37it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 4/16 [00:02<00:08,  1.46it/s]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 5/16 [00:03<00:07,  1.52it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 6/16 [00:05<00:09,  1.03it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 7/16 [00:05<00:07,  1.13it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 8/16 [00:06<00:07,  1.14it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 9/16 [00:07<00:05,  1.25it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 10/16 [00:07<00:04,  1.35it/s]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 11/16 [00:08<00:03,  1.37it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 12/16 [00:09<00:02,  1.46it/s]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 13/16 [00:09<00:02,  1.36it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 14/16 [00:10<00:01,  1.43it/s]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 15/16 [00:11<00:00,  1.49it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 16/16 [00:11<00:00,  1.88it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 16/16 [00:11<00:00,  1.39it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:25 [loader.py:458] Loading weights took 12.03 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:25 [gpu_model_runner.py:1291] Model loading took 14.3001 GiB and 12.707204 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m INFO 08-11 23:54:26 [loader.py:458] Loading weights took 12.61 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m INFO 08-11 23:54:26 [gpu_model_runner.py:1291] Model loading took 14.3001 GiB and 13.652692 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:26 [loader.py:458] Loading weights took 12.86 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:27 [gpu_model_runner.py:1291] Model loading took 14.3001 GiB and 14.169387 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:54:27 [loader.py:458] Loading weights took 13.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:54:28 [gpu_model_runner.py:1291] Model loading took 14.3001 GiB and 15.316636 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m INFO 08-11 23:54:37 [backends.py:416] Using cache directory: /gpfs01/home/ppxac9/.cache/vllm/torch_compile_cache/d5af444735/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m INFO 08-11 23:54:37 [backends.py:426] Dynamo bytecode transform time: 9.35 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m INFO 08-11 23:54:39 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:42 [backends.py:416] Using cache directory: /gpfs01/home/ppxac9/.cache/vllm/torch_compile_cache/d5af444735/rank_2_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:42 [backends.py:426] Dynamo bytecode transform time: 14.36 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:54:44 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:54:46 [backends.py:416] Using cache directory: /gpfs01/home/ppxac9/.cache/vllm/torch_compile_cache/d5af444735/rank_3_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:54:46 [backends.py:426] Dynamo bytecode transform time: 18.33 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:46 [backends.py:416] Using cache directory: /gpfs01/home/ppxac9/.cache/vllm/torch_compile_cache/d5af444735/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:54:46 [backends.py:426] Dynamo bytecode transform time: 18.65 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:54:48 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 08-11 23:54:48 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m WARNING 08-11 23:55:02 [fused_moe.py:670] Using default MoE config. Performance might be sub-optimal! Config file not found at /gpfs01/home/ppxac9/MLiS-Placement-Thesis/pipeline/optimizing_inference/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100_80GB_PCIe.json\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m WARNING 08-11 23:55:02 [fused_moe.py:670] Using default MoE config. Performance might be sub-optimal! Config file not found at /gpfs01/home/ppxac9/MLiS-Placement-Thesis/pipeline/optimizing_inference/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100_80GB_PCIe.json\n",
      "WARNING 08-11 23:55:02 [fused_moe.py:670] Using default MoE config. Performance might be sub-optimal! Config file not found at /gpfs01/home/ppxac9/MLiS-Placement-Thesis/pipeline/optimizing_inference/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100_80GB_PCIe.json\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m WARNING 08-11 23:55:02 [fused_moe.py:670] Using default MoE config. Performance might be sub-optimal! Config file not found at /gpfs01/home/ppxac9/MLiS-Placement-Thesis/pipeline/optimizing_inference/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100_80GB_PCIe.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:55:02 [monitor.py:33] torch.compile takes 18.65 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m INFO 08-11 23:55:02 [monitor.py:33] torch.compile takes 18.33 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:55:02 [monitor.py:33] torch.compile takes 14.36 s in total\n",
      "INFO 08-11 23:55:02 [monitor.py:33] torch.compile takes 9.35 s in total\n",
      "INFO 08-11 23:55:07 [kv_cache_utils.py:634] GPU KV cache size: 2,395,376 tokens\n",
      "INFO 08-11 23:55:07 [kv_cache_utils.py:637] Maximum concurrency for 262,144 tokens per request: 9.14x\n",
      "INFO 08-11 23:55:07 [kv_cache_utils.py:634] GPU KV cache size: 2,395,024 tokens\n",
      "INFO 08-11 23:55:07 [kv_cache_utils.py:637] Maximum concurrency for 262,144 tokens per request: 9.14x\n",
      "INFO 08-11 23:55:07 [kv_cache_utils.py:634] GPU KV cache size: 2,395,024 tokens\n",
      "INFO 08-11 23:55:07 [kv_cache_utils.py:637] Maximum concurrency for 262,144 tokens per request: 9.14x\n",
      "INFO 08-11 23:55:07 [kv_cache_utils.py:634] GPU KV cache size: 2,396,736 tokens\n",
      "INFO 08-11 23:55:07 [kv_cache_utils.py:637] Maximum concurrency for 262,144 tokens per request: 9.14x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3301010)\u001b[0;0m INFO 08-11 23:56:18 [gpu_model_runner.py:1626] Graph capturing finished in 71 secs, took 1.89 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=3301077)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=3301047)\u001b[0;0m INFO 08-11 23:56:18 [gpu_model_runner.py:1626] Graph capturing finished in 72 secs, took 1.89 GiB\n",
      "INFO 08-11 23:56:18 [gpu_model_runner.py:1626] Graph capturing finished in 72 secs, took 1.89 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3300986)\u001b[0;0m INFO 08-11 23:56:19 [gpu_model_runner.py:1626] Graph capturing finished in 72 secs, took 1.89 GiB\n",
      "INFO 08-11 23:56:19 [core.py:163] init engine (profile, create kv cache, warmup model) took 110.87 seconds\n",
      "INFO 08-11 23:56:19 [core_client.py:435] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "        model=\"Qwen/Qwen3-30B-A3B-Instruct-2507\",\n",
    "        tensor_parallel_size=4\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29e198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e82647-69d5-4e70-a36f-0934a95e3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the coup capital?\"\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a3babd-ecec-4933-b717-096586448a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:18<00:00,  9.31s/it, est. speed input: 0.70 toks/s, output: 10.74 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2282e610-d3c0-465f-ae77-daf7d8fdb696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'What is the capital of France?', Generated text: ' Also, can you explain the significance of this city in French history and culture?\\n\\nThe capital of France is Paris. Paris has been the political, cultural, and economic center of France for centuries. It played a crucial role in the French Revolution, which began in 1789 and led to the overthrow of the monarchy. Paris is also home to iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. The city has been a hub for art'\n",
      "Prompt: 'What is the coup capital?', Generated text: '  Is it a verb?\\n\\nThe phrase **\"coup capital\"** is not a standard or commonly used term in English, and it is **not a verb**. Let\\'s break it down:\\n\\n### 1. **\"Coup\"**:\\n- As a **noun**, \"coup\" refers to a sudden, decisive, and often violent or illegal seizure of power, especially in a government (e.g., *a military coup*).\\n- As a **verb**, \"to'\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94fb72ed-f84a-4e2b-b7f3-8a03d3d66cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, RootModel\n",
    "from typing import Union, Literal\n",
    "from enum import Enum\n",
    "\n",
    "# Separate enums for clarity and type safety\n",
    "class MathOperator(str, Enum):\n",
    "    ADD = \"ADD\"\n",
    "    MULTIPLY = \"MULTIPLY\"\n",
    "    DIVIDE = \"DIVIDE\"\n",
    "    CEIL = \"CEIL\"\n",
    "    \n",
    "class Comparator(str, Enum):\n",
    "    GREATER_THAN = \"GREATER_THAN\"\n",
    "    LESS_THAN = \"LESS_THAN\"\n",
    "    EQUAL_TO = \"EQUAL_TO\"\n",
    "\n",
    "class Units(str, Enum):\n",
    "    HOURS = \"HOURS\"\n",
    "    MINUTES = \"MINUTES\"\n",
    "    EUROS = \"EUROS\"\n",
    "    PERCENT = \"PERCENT\"\n",
    "    UNITLESS = \"UNITLESS\"\n",
    "    \n",
    "# --- Node Definitions ---\n",
    "\n",
    "class ValueNode(BaseModel):\n",
    "    type: Literal[\"VALUE\"] = \"VALUE\"\n",
    "    value: float\n",
    "    description: str\n",
    "    unit: Units\n",
    "\n",
    "class VariableNode(BaseModel):\n",
    "    type: Literal[\"VARIABLE\"] = \"VARIABLE\"\n",
    "    name: str \n",
    "    description: str\n",
    "    unit: Units\n",
    "\n",
    "class BinaryOpNode(BaseModel):\n",
    "    \"\"\"Node for mathematical operations that produce a number.\"\"\"\n",
    "    type: Literal[\"BINARY_OPERATION\"] = \"BINARY_OPERATION\"\n",
    "    operator: MathOperator\n",
    "    left: 'AnyNode'\n",
    "    right: 'AnyNode'\n",
    "\n",
    "class ComparisonNode(BaseModel):\n",
    "    \"\"\"Node for comparison operations that produce a boolean.\"\"\"\n",
    "    type: Literal[\"COMPARISON\"] = \"COMPARISON\"\n",
    "    operator: Comparator\n",
    "    left: 'AnyNode'\n",
    "    right: 'AnyNode'\n",
    "\n",
    "class ConditionalNode(BaseModel):\n",
    "    \"\"\"Node for if-then-else logic.\"\"\"\n",
    "    type: Literal[\"CONDITIONAL\"] = \"CONDITIONAL\"\n",
    "    condition: ComparisonNode # Condition must be a comparison\n",
    "    if_true: 'AnyNode'\n",
    "    if_false: 'AnyNode'\n",
    "\n",
    "# --- Recursive Setup ---\n",
    "\n",
    "AnyNode = Union[\n",
    "    ValueNode, \n",
    "    VariableNode, \n",
    "    BinaryOpNode, \n",
    "    ConditionalNode\n",
    "]\n",
    "\n",
    "# Use model_rebuild() to safely resolve all forward references\n",
    "BinaryOpNode.model_rebuild()\n",
    "ConditionalNode.model_rebuild()\n",
    "ComparisonNode.model_rebuild()\n",
    "\n",
    "class Node(RootModel):\n",
    "    root: BinaryOpNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2c5dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = Node.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6beaaef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "guided_decoding_params = GuidedDecodingParams(json=json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76edc6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(guided_decoding=guided_decoding_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "874974a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Dict, Any, Optional\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class ParameterStatus(str, Enum):\n",
    "    \"\"\"An enumeration for clear, explicit parameter statuses.\"\"\"\n",
    "    KNOWN = \"KNOWN\"\n",
    "    SYMBOLIC = \"SYMBOLIC\"\n",
    "\n",
    "class ParameterDetail(BaseModel):\n",
    "    \"\"\"A structured model to describe each parameter identified from the query.\"\"\"\n",
    "    name: str = Field(..., description=\"The name of the parameter.\")\n",
    "    status: ParameterStatus = Field(..., description=\"Whether the parameter's value is known from the query or is a symbolic variable.\")\n",
    "    value: Optional[Any] = Field(None, description=\"The actual value of the parameter, if its status is 'KNOWN'. Must be null if status is 'SYMBOLIC'.\")\n",
    "\n",
    "\n",
    "class ReasoningSchemaStep1(BaseModel):\n",
    "    query_parameters: List[ParameterDetail] = Field(\n",
    "        ...,\n",
    "        description=\"A structured list of all parameters identified from the query and their status.\"\n",
    "    )\n",
    "class ReasoningSchemaStep2(BaseModel):\n",
    "    \"\"\"\n",
    "    A simplified schema for Step 2 that captures all constants and rules as a simple list of descriptive strings.\n",
    "    \"\"\"\n",
    "    identified_constants_and_rules: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"A comprehensive list of all facts, constants, and conditional rules extracted from the document that are necessary for the final calculation. Each string in the list should be a self-contained, clear statement. For example: 'The rate for a Narrow Satellite stand is €32.90 per 15 minutes' or 'A 100% surcharge is applied if parking duration is between 48 and 72 hours'.\"\n",
    "    )\n",
    "class ReasoningSchemaStep3(BaseModel):\n",
    "    synthesis_plan: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise, step-by-step plan describing how the variables and constants are combined into the final computation graph.\"\n",
    "    )\n",
    "class ReasoningSchemaStep4(BaseModel):\n",
    "    rethink: str = Field(\n",
    "        ...,\n",
    "        description=\"Final check to ensure the plan correctly uses variables and constants and handles all logic from the document.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(\n",
    "    prompts=\"Classify this sentiment: vLLM is wonderful!\",\n",
    "    sampling_params=sampling_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0627a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic import BaseModel\n",
    "import guidance\n",
    "litellm_desc = {\n",
    "    \"model_name\": \"Qwen/Qwen3-1.7B\",\n",
    "    \"litellm_params\": {  # params for litellm completion/embedding call\n",
    "        \"model\": \"hosted_vllm/Qwen/Qwen3-1.7B\",\n",
    "        \"api_key\": os.environ.get(\"VLLM_API_KEY\", \"NO_KEY\"), # set your vLLM API key if needed\n",
    "        \"api_base\": \"http://localhost:8000/v1\", # change to your vLLM API base URL\n",
    "    },\n",
    "}\n",
    "base_lm = guidance.models.experimental.LiteLLM(model_description=litellm_desc, echo=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
