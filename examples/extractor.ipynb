{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50215638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from mlisplacement.structs import airports\n",
    "from mlisplacement.structs import reasoning\n",
    "from mlisplacement.extractor.chat_templates import QwenChatTemplate\n",
    "from mlisplacement.extractor.reasoning import generate\n",
    "from mlisplacement.utils import load_model, read_md\n",
    "\n",
    "# from guidance import models, system, user, assistant, json as gen_json, gen\n",
    "import torch\n",
    "import json\n",
    "import torch\n",
    "from enum import Enum\n",
    "import guidance\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.path.join(\"data\")\n",
    "filepath = os.path.join(datadir, \"synthdoc_example.md\")\n",
    "# MODEL_ID = \"Qwen/Qwen3-30B-A3B\"\n",
    "# MODEL_ID = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\" #This already has memory error\n",
    "# MODEL_ID = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model, hf_tokenizer = load_model(MODEL_ID, low_cpu_mem_usage=True)\n",
    "tok = hf_tokenizer\n",
    "chat_template = QwenChatTemplate\n",
    "model = guidance.models.Transformers(hf_model, tok, chat_template=chat_template)\n",
    "charges = airports.AirportCharges()\n",
    "variables = airports.AirportVariables()\n",
    "input_md = read_md(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "def generate(model, query, charge_name, markdown_content, all_charges, all_variables):\n",
    "    graph_builder = ComputationGraphBuilder(model=model)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    llm_structured_response = graph_builder.build(\n",
    "        document_content=markdown_content,\n",
    "        query=query,\n",
    "        charge_name=charge_name,\n",
    "        all_charges=all_charges,\n",
    "        all_variables=all_variables\n",
    "    )\n",
    "    end_time = time.perf_counter()\n",
    "    build_time = end_time - start_time\n",
    "\n",
    "    return llm_structured_response, build_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eaa826",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Calculate the total security charge\"\n",
    "charge_name = \"security_fee\"\n",
    "\n",
    "res_model, build_time = generate(model, query, charge_name=charge_name, markdown_content=input_md, all_charges=charges.to_dict(), all_variables=variables.to_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlisp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
